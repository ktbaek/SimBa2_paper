{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import PDB \n",
    "from Bio.PDB import PDBParser\n",
    "from simba2 import methods as simba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training and test datasets\n",
    "b1131 = pd.read_csv('../data/processed/B1131_expddg.csv')\n",
    "b663 =  pd.read_csv('../data/processed/B663_expddg.csv')\n",
    "s350 =   pd.read_csv('../data/processed/S350_expddg.csv')\n",
    "\n",
    "# Convert No column to string for compatibility with simba output\n",
    "b1131.No = b1131.No.astype('str')\n",
    "b663.No = b663.No.astype('str')\n",
    "s350.No = s350.No.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_dir = '../data/external/PDB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe containing unique entries across datasets\n",
    "dataset_residues = (pd.concat([b1131, b663, s350])\n",
    " .drop(columns = ['exp_ddG'])\n",
    " .drop_duplicates())\n",
    "\n",
    "# List of unique PDBs across datasets\n",
    "dataset_pdbs = dataset_residues['PDB'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-bearing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate RSA, Hdiff, Vdiff, and predicted ddG (predicted ddG is not used for creating datasets) with Simba2 \n",
    "# for each PDB in datasets and indicate if those entries are present in the datasets.\n",
    "# Insert column indicating if PDB is a heterooligomer\n",
    "df_list = []\n",
    "for pdb in dataset_pdbs:\n",
    "    pdb_path, exists = simba.exists_pdb(pdb, pdb_dir)\n",
    "    print(pdb_path, exists)\n",
    "    if exists:\n",
    "        df = simba.simba2_predict(pdb, pdb_path)\n",
    "        multichain, homo = simba.check_chains(df)\n",
    "        df.insert(loc=1, column='Hetero', value=multichain and not homo)\n",
    "        df_list.append(pd.merge(df, \n",
    "                 dataset_residues[dataset_residues['PDB'] == pdb],\n",
    "                 how = 'outer',\n",
    "                 indicator = True,       \n",
    "                 left_on = ['PDB', 'Number', 'Wild', 'Mutated'],\n",
    "                 right_on = ['PDB', 'No', 'Wild', 'Mutated']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "simba_df = pd.concat(df_list)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_unique(inputlist):\n",
    "    return len(inputlist.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which residues in the datasets that were not merged with Simba output\n",
    "\n",
    "absent = simba_df[simba_df['_merge'] == 'right_only']\n",
    "print('B1131:', pd.merge(b1131, absent, on = ['PDB', 'Wild', 'No', 'Mutated'], how = 'inner'))\n",
    "print('B663:', pd.merge(b663, absent, on = ['PDB', 'Wild', 'No', 'Mutated'], how = 'inner'))\n",
    "print('S350:', pd.merge(s350, absent, on = ['PDB', 'Wild', 'No', 'Mutated'], how = 'inner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-worse",
   "metadata": {},
   "source": [
    "Four data points in B663 does not exist in the simba output (the residue type is not at that position in the PDBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "b1131 = b1131[(b1131['exp_ddG'] > -8) & (b1131['exp_ddG'] < 8)]\n",
    "b663 = b663[(b663['exp_ddG'] > -8) & (b663['exp_ddG'] < 8)]\n",
    "s350 = s350[(s350['exp_ddG'] > -8) & (s350['exp_ddG'] < 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b1131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b663)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with those entries in the dataset where it is not\n",
    "# possible to know which chain it is located in. \n",
    "\n",
    "## keep only mutations that are present in the data sets\n",
    "simba_df = simba_df[simba_df['_merge'] == 'both'].drop(columns = ['_merge'])\n",
    "\n",
    "## keep heterooligomers\n",
    "hetero = simba_df[simba_df['Hetero']] \n",
    "hetero[['PDB','Number', 'Mutated']].drop_duplicates() \n",
    "\n",
    "## for each unique mutation, count chains\n",
    "no_chains = pd.DataFrame(hetero.groupby(['PDB', 'Number', 'Mutated'])['Chain'].agg(len_unique))\n",
    "\n",
    "## make dataframe of mutations with more than one chain\n",
    "unsure_chain = no_chains[no_chains['Chain'] != 1]\n",
    "unsure_chain = pd.DataFrame(list(unsure_chain.index), columns = ['PDB', 'No', 'Mutated'])\n",
    "\n",
    "unsure_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-album",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hetero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-michael",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-period",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def anti_join(dataset, unsure):\n",
    "    df = pd.merge(dataset, unsure, on = ['PDB', 'No', 'Mutated'], how = 'outer', indicator = True)\n",
    "    return df[df['_merge'] == 'left_only'].drop(columns = ['_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard entries where it's not possible to know the chain\n",
    "b1131 = anti_join(b1131, unsure_chain)\n",
    "b663 = anti_join(b663, unsure_chain)\n",
    "s350 = anti_join(s350, unsure_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b1131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b663)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "simba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets (without unsure chains) with simba output and tidy up result \n",
    "def choose_var(row, variable):\n",
    "    if pd.isna(row[variable + '_mean']):\n",
    "        return row[variable]\n",
    "    else:\n",
    "        return row[variable + '_mean']\n",
    "\n",
    "def finalize_dataset(dataset, simba_output):\n",
    "    df = pd.merge(dataset, simba_output, on = ['PDB', 'No', 'Wild', 'Mutated'], how = 'inner')\n",
    "    df['final_RSA'] = df.apply(lambda row : choose_var(row, 'RSA'), axis=1)\n",
    "    df = df[['PDB', 'Wild', 'Number', 'final_RSA', 'Mutated', 'exp_ddG', 'Hdiff', 'Vdiff']]\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.rename(columns = {\"final_RSA\" : \"RSA\"})\n",
    "    df = df[['PDB', 'Wild', 'Number', 'RSA', 'Mutated', 'Hdiff', 'Vdiff', 'exp_ddG']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-advisory",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b1131_simba2 = finalize_dataset(b1131, simba_df)\n",
    "#b1131_simba2.to_csv('../data/b1131_simba2.csv', index = False)\n",
    "\n",
    "b663_simba2 = finalize_dataset(b663, simba_df)\n",
    "#b663_simba2.to_csv('../data/b663_simba2.csv', index = False)\n",
    "\n",
    "s350_simba2 = finalize_dataset(s350, simba_df)\n",
    "#s350_simba2.to_csv('../data/s350_simba2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After removal of hetero:\", len(b1131))\n",
    "print(\"Final:\", len(b1131_simba2))\n",
    "print(\"Duplicates:\", sum(b1131.duplicated()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-renaissance",
   "metadata": {},
   "source": [
    "There are two duplicated data points in the original B1131 which are removed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The duplicated entries are:\n",
    "b1131[b1131.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-circle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"After removal of hetero:\", len(b663))\n",
    "print(\"Final:\", len(b663_simba2))\n",
    "print(\"Duplicates:\", sum(b663.duplicated()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-episode",
   "metadata": {},
   "source": [
    "There were four missing data points in the PDBs, see above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-worship",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"After removal of hetero:\", len(s350))\n",
    "print(\"Final:\", len(s350_simba2))\n",
    "print(\"Duplicates:\", sum(s350.duplicated()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-serial",
   "metadata": {},
   "source": [
    "One missing data point due to obsolete PDB 2A01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets as new datasets with new names\n",
    "\n",
    "b1131_simba2['dataset'] = \"B1112\"\n",
    "b663_simba2['dataset'] = \"B655\"\n",
    "s350_simba2['dataset'] = \"S344\"\n",
    "\n",
    "b1131_simba2.to_csv('../data/processed/B1112.csv', index = False)\n",
    "b663_simba2.to_csv('../data/processed/B655.csv', index = False)\n",
    "s350_simba2.to_csv('../data/processed/S344.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simba2 import methods as simba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(methods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
